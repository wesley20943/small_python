{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "炎龍老師week6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdOwMMVE_dIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbTX3yz_w70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKb9sh_t_x7g",
        "colab_type": "code",
        "outputId": "157163a5-fe1a-4740-d740-f9f3615e7190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "(x_train_o, y_train_o), (x_test_o, y_test_o) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30G9Jlhp_7-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdo_Cj4V__ie",
        "colab_type": "code",
        "outputId": "1bd5eafb-ec4b-4b27-8a5b-6ecf3302b2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_o.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovZ4BR8EAc83",
        "colab_type": "code",
        "outputId": "cc7b5891-7515-4d43-bb7a-a68843226b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_o.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQu7fBkAfPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnY_nIMrApkQ",
        "colab_type": "code",
        "outputId": "58011ff6-0b2d-4b22-a3cc-7aa546f055b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fik40DeCArMa",
        "colab_type": "code",
        "outputId": "3f43f48e-e81c-4968-f19f-0ebf8e00748e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSH0-oyLAsoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = tf.keras.utils.to_categorical(y_test_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjBt4Qo4Bi_w",
        "colab_type": "code",
        "outputId": "d5f26765-6ceb-4d72-b491-242e78e38de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(x_train_o[0], cmap=\"Greys\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f01045ea1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcId1YnaA1Fh",
        "colab_type": "code",
        "outputId": "56bf25f0-796a-43cd-e7b2-949625431fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_o[0].sum()/(28*28)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.25382653061224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP0TnrdRA_lW",
        "colab_type": "code",
        "outputId": "dfc9c5ff-7dca-484f-c601-eb06ea55e551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_o[0].std()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101.79234620324726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1saAuy2Bex8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.zeros([60000,28,28])\n",
        "for i in range(60000):\n",
        "  x_train[i] =( x_train_o[i] - (x_train_o[i].sum()/(28*28))  ) / x_train_o[i].std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqzEx1M5EDWD",
        "colab_type": "code",
        "outputId": "df6861d2-a90c-42a4-e7e9-13833a76dd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EODBwKw2EpSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(60000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLBVuYweEHIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eu-_98cEhy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add( tf.keras.layers.Conv2D(256, (8,8), padding = \"same\", input_shape = (28,28,1), activation=tf.keras.layers.LeakyReLU(alpha=0.3) )   )\n",
        "model.add( tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(rate = 0.4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4IpMfQ9FmtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUy1Ltz3F3Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add( tf.keras.layers.Conv2D(128, (3,3), padding = \"same\", input_shape = (28,28,1), activation=tf.keras.layers.LeakyReLU(alpha=0.3) )   )\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add( tf.keras.layers.Conv2D(64, (3,3), padding = \"same\", input_shape = (28,28,1), activation=tf.keras.layers.LeakyReLU(alpha=0.3) )   )\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add( tf.keras.layers.Conv2D(32, (1,1), padding = \"same\", input_shape = (28,28,1), activation=tf.keras.layers.LeakyReLU(alpha=0.3) )   )\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add( tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(tf.keras.layers.Dropout(rate = 0.3))\n",
        "\n",
        "model.add( tf.keras.layers.Conv2D(64, (3,3), padding = \"same\", input_shape = (28,28,1), activation=tf.keras.layers.LeakyReLU(alpha=0.3) )   )\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add( tf.keras.layers.Conv2D(32, (1,1), padding = \"same\", input_shape = (28,28,1), activation=tf.keras.layers.LeakyReLU(alpha=0.3) )   )\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add( tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(tf.keras.layers.Dropout(rate = 0.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EujrxxRHIZWq",
        "colab_type": "code",
        "outputId": "b7dc5137-987c-4455-f3ff-7117f90140f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 256)       16640     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 32)        2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 32)          2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 32)          0         \n",
            "=================================================================\n",
            "Total params: 410,432\n",
            "Trainable params: 409,280\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_d2ZZAUIaoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgUBP5L2Iq-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dense(32, input_dim = 288 ,activation = \"relu\" ))\n",
        "model.add(tf.keras.layers.Dropout(rate = 0.5))\n",
        "model.add(tf.keras.layers.Dense(10,activation = \"softmax\" ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwb7j5FYJSLx",
        "colab_type": "code",
        "outputId": "9904fb5c-5786-4028-bb9c-1447db846135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 256)       16640     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 32)        2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 32)          2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                9248      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 420,010\n",
            "Trainable params: 418,858\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXRrqZf-JUHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlpUZgkeJa7t",
        "colab_type": "code",
        "outputId": "29fb6a60-622b-4b8f-e087-d009ba93e68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size = 300, epochs= 128)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.2029 - accuracy: 0.9285\n",
            "Epoch 2/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.2019 - accuracy: 0.9295\n",
            "Epoch 3/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1994 - accuracy: 0.9291\n",
            "Epoch 4/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1978 - accuracy: 0.9308\n",
            "Epoch 5/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.2016 - accuracy: 0.9299\n",
            "Epoch 6/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.2000 - accuracy: 0.9292\n",
            "Epoch 7/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1963 - accuracy: 0.9304\n",
            "Epoch 8/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1970 - accuracy: 0.9299\n",
            "Epoch 9/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1966 - accuracy: 0.9300\n",
            "Epoch 10/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1962 - accuracy: 0.9307\n",
            "Epoch 11/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1970 - accuracy: 0.9298\n",
            "Epoch 12/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1946 - accuracy: 0.9311\n",
            "Epoch 13/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1964 - accuracy: 0.9304\n",
            "Epoch 14/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1975 - accuracy: 0.9302\n",
            "Epoch 15/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1984 - accuracy: 0.9300\n",
            "Epoch 16/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1997 - accuracy: 0.9299\n",
            "Epoch 17/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1915 - accuracy: 0.9323\n",
            "Epoch 18/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1974 - accuracy: 0.9323\n",
            "Epoch 19/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1943 - accuracy: 0.9315\n",
            "Epoch 20/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1913 - accuracy: 0.9320\n",
            "Epoch 21/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1921 - accuracy: 0.9318\n",
            "Epoch 22/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1887 - accuracy: 0.9330\n",
            "Epoch 23/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1911 - accuracy: 0.9315\n",
            "Epoch 24/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1945 - accuracy: 0.9310\n",
            "Epoch 25/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1882 - accuracy: 0.9335\n",
            "Epoch 26/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1893 - accuracy: 0.9340\n",
            "Epoch 27/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1926 - accuracy: 0.9321\n",
            "Epoch 28/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1905 - accuracy: 0.9332\n",
            "Epoch 29/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1889 - accuracy: 0.9326\n",
            "Epoch 30/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1891 - accuracy: 0.9330\n",
            "Epoch 31/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1893 - accuracy: 0.9335\n",
            "Epoch 32/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1891 - accuracy: 0.9357\n",
            "Epoch 33/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1877 - accuracy: 0.9346\n",
            "Epoch 34/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1902 - accuracy: 0.9343\n",
            "Epoch 35/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1844 - accuracy: 0.9338\n",
            "Epoch 36/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1866 - accuracy: 0.9360\n",
            "Epoch 37/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1857 - accuracy: 0.9344\n",
            "Epoch 38/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1898 - accuracy: 0.9335\n",
            "Epoch 39/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1835 - accuracy: 0.9355\n",
            "Epoch 40/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1880 - accuracy: 0.9334\n",
            "Epoch 41/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1863 - accuracy: 0.9340\n",
            "Epoch 42/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1846 - accuracy: 0.9340\n",
            "Epoch 43/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1835 - accuracy: 0.9354\n",
            "Epoch 44/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1819 - accuracy: 0.9358\n",
            "Epoch 45/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1849 - accuracy: 0.9348\n",
            "Epoch 46/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1827 - accuracy: 0.9356\n",
            "Epoch 47/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1766 - accuracy: 0.9386\n",
            "Epoch 48/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1810 - accuracy: 0.9371\n",
            "Epoch 49/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1842 - accuracy: 0.9365\n",
            "Epoch 50/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1832 - accuracy: 0.9364\n",
            "Epoch 51/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1838 - accuracy: 0.9363\n",
            "Epoch 52/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1785 - accuracy: 0.9367\n",
            "Epoch 53/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1782 - accuracy: 0.9383\n",
            "Epoch 54/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1770 - accuracy: 0.9372\n",
            "Epoch 55/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1809 - accuracy: 0.9353\n",
            "Epoch 56/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1779 - accuracy: 0.9369\n",
            "Epoch 57/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1771 - accuracy: 0.9362\n",
            "Epoch 58/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1806 - accuracy: 0.9368\n",
            "Epoch 59/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1785 - accuracy: 0.9373\n",
            "Epoch 60/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1766 - accuracy: 0.9376\n",
            "Epoch 61/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1761 - accuracy: 0.9383\n",
            "Epoch 62/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1764 - accuracy: 0.9378\n",
            "Epoch 63/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1761 - accuracy: 0.9387\n",
            "Epoch 64/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1731 - accuracy: 0.9394\n",
            "Epoch 65/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1746 - accuracy: 0.9387\n",
            "Epoch 66/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1761 - accuracy: 0.9391\n",
            "Epoch 67/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1762 - accuracy: 0.9388\n",
            "Epoch 68/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1735 - accuracy: 0.9390\n",
            "Epoch 69/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1743 - accuracy: 0.9399\n",
            "Epoch 70/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1744 - accuracy: 0.9395\n",
            "Epoch 71/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1743 - accuracy: 0.9388\n",
            "Epoch 72/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1738 - accuracy: 0.9387\n",
            "Epoch 73/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1743 - accuracy: 0.9400\n",
            "Epoch 74/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1725 - accuracy: 0.9392\n",
            "Epoch 75/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1723 - accuracy: 0.9393\n",
            "Epoch 76/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1767 - accuracy: 0.9382\n",
            "Epoch 77/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1729 - accuracy: 0.9395\n",
            "Epoch 78/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1722 - accuracy: 0.9395\n",
            "Epoch 79/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1701 - accuracy: 0.9400\n",
            "Epoch 80/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1703 - accuracy: 0.9408\n",
            "Epoch 81/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1701 - accuracy: 0.9405\n",
            "Epoch 82/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1687 - accuracy: 0.9410\n",
            "Epoch 83/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1716 - accuracy: 0.9401\n",
            "Epoch 84/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1677 - accuracy: 0.9416\n",
            "Epoch 85/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1719 - accuracy: 0.9401\n",
            "Epoch 86/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1687 - accuracy: 0.9410\n",
            "Epoch 87/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1679 - accuracy: 0.9408\n",
            "Epoch 88/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1682 - accuracy: 0.9406\n",
            "Epoch 89/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1652 - accuracy: 0.9433\n",
            "Epoch 90/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1679 - accuracy: 0.9402\n",
            "Epoch 91/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1676 - accuracy: 0.9403\n",
            "Epoch 92/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1662 - accuracy: 0.9400\n",
            "Epoch 93/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1691 - accuracy: 0.9408\n",
            "Epoch 94/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1637 - accuracy: 0.9423\n",
            "Epoch 95/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1666 - accuracy: 0.9409\n",
            "Epoch 96/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1644 - accuracy: 0.9421\n",
            "Epoch 97/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1662 - accuracy: 0.9415\n",
            "Epoch 98/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1660 - accuracy: 0.9407\n",
            "Epoch 99/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1643 - accuracy: 0.9424\n",
            "Epoch 100/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1638 - accuracy: 0.9436\n",
            "Epoch 101/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1641 - accuracy: 0.9430\n",
            "Epoch 102/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1670 - accuracy: 0.9419\n",
            "Epoch 103/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1645 - accuracy: 0.9416\n",
            "Epoch 104/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1596 - accuracy: 0.9431\n",
            "Epoch 105/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1633 - accuracy: 0.9433\n",
            "Epoch 106/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1621 - accuracy: 0.9430\n",
            "Epoch 107/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1653 - accuracy: 0.9412\n",
            "Epoch 108/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1623 - accuracy: 0.9423\n",
            "Epoch 109/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1597 - accuracy: 0.9438\n",
            "Epoch 110/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1659 - accuracy: 0.9420\n",
            "Epoch 111/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1628 - accuracy: 0.9432\n",
            "Epoch 112/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1616 - accuracy: 0.9437\n",
            "Epoch 113/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1609 - accuracy: 0.9436\n",
            "Epoch 114/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1584 - accuracy: 0.9441\n",
            "Epoch 115/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1571 - accuracy: 0.9452\n",
            "Epoch 116/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1582 - accuracy: 0.9445\n",
            "Epoch 117/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1649 - accuracy: 0.9428\n",
            "Epoch 118/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1584 - accuracy: 0.9452\n",
            "Epoch 119/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1598 - accuracy: 0.9436\n",
            "Epoch 120/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1564 - accuracy: 0.9446\n",
            "Epoch 121/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1582 - accuracy: 0.9448\n",
            "Epoch 122/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1548 - accuracy: 0.9456\n",
            "Epoch 123/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1601 - accuracy: 0.9439\n",
            "Epoch 124/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1612 - accuracy: 0.9431\n",
            "Epoch 125/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1585 - accuracy: 0.9446\n",
            "Epoch 126/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1560 - accuracy: 0.9450\n",
            "Epoch 127/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1538 - accuracy: 0.9471\n",
            "Epoch 128/128\n",
            "200/200 [==============================] - 7s 34ms/step - loss: 0.1590 - accuracy: 0.9445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAsasoZtJ3AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = np.zeros([10000,28,28])\n",
        "for i in range(10000):\n",
        "  x_test[i] =( x_test_o[i] - (x_test_o[i].sum()/(28*28))  ) / x_test_o[i].std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyEB1fE_X8FN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fb31c1b-49e3-4fef-d310-46faabe77e4d"
      },
      "source": [
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "score = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2511 - accuracy: 0.9269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfAwULplYF_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4qV23oPYVh1",
        "colab_type": "text"
      },
      "source": [
        "## Model structure: <br />\n",
        " #### conv 8*8 , 256<br />\n",
        "    Maxpooling 2*2\n",
        "    Normaliztion\n",
        "    dropout\n",
        "\n",
        " #### conv 3*3 , 128<br />\n",
        "    Normaliztion\n",
        "\n",
        " #### conv 3*3 , 64, <br /> \n",
        "    Normaliztion\n",
        " #### conv 1*1 , 32<br />\n",
        "    Maxpooling 2*2\n",
        "    dropout\n",
        " #### conv 3*3 , 64, <br /> \n",
        "    Normaliztion\n",
        " #### conv 1*1 , 32<br />\n",
        "    Maxpooling 2*2\n",
        "    dropout\n",
        " #### FNN 32<br />\n",
        "    dropout\n",
        " #### FNN 10<br />\n",
        " #### Adam, Crossentrophy\n",
        "\n",
        "batchsize -> 50-> 80-> 100-> 200-> 300<br />\n",
        "epochs    -> 20-> 20-> 30->  50 -> 128\n"
      ]
    }
  ]
}